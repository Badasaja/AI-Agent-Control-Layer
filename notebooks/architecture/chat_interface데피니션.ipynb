{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08610c08",
   "metadata": {},
   "source": [
    "### CIF/state.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b36820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "State management for the langraph application.\n",
    "This provides the main state for dependency injection and state management.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Any\n",
    "from langgraph.graph import add_messages\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\"\"\"\n",
    "State management for the vanilla code generation application.\n",
    "This provides the main context for dependency injection and state management.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "# Data classes for code generation process\n",
    "@dataclass\n",
    "class TestFailure:\n",
    "    \"\"\"Represents a single test failure\"\"\"\n",
    "    name: str\n",
    "    trace: str\n",
    "\n",
    "@dataclass\n",
    "class TestResults:\n",
    "    \"\"\"Results from running tests\"\"\"\n",
    "    passed: int = 0\n",
    "    failed: int = 0\n",
    "    failures: List[TestFailure] = field(default_factory=list)\n",
    "    test_explanation: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class CodeResult:\n",
    "    \"\"\"Results from code generation\"\"\"\n",
    "    code: str = \"\"\n",
    "    code_explanation: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CodeGenState:\n",
    "    \"\"\"\n",
    "    State object for code generation agents.\n",
    "    This manages the state for the code generation process.\n",
    "    \"\"\"\n",
    "    # User input for code generation\n",
    "    user_prompt_for_app: Optional[str] = None\n",
    "    \n",
    "    # Constraints for code generation\n",
    "    constraints: Dict[str, str] = field(default_factory=lambda: {\n",
    "        \"language\": \"Python 3.8+\",\n",
    "        \"style\": \"PEP 8 compliant\",\n",
    "        \"documentation\": \"Add docstrings\",\n",
    "        \"allowed_packages\": \"stdlib\"\n",
    "    })\n",
    "    \n",
    "    # Configuration\n",
    "    package_name: str = \"app\"\n",
    "    max_iterations: int = 3\n",
    "    max_tests: int = 8\n",
    "    \n",
    "    # Runtime state\n",
    "    iteration: int = 0\n",
    "    process_completed: bool = False\n",
    "    \n",
    "    current_code: Optional[Any] = None  \n",
    "    previous_code: Optional[str] = None\n",
    "    test_results: Optional[Any] = None  \n",
    "    review_notes: List[str] = field(default_factory=list)\n",
    "\n",
    "    \n",
    "    \n",
    "def codegen_increment_iteration(code_gen_state: CodeGenState) -> None:\n",
    "    \"\"\"Increment the iteration counter for code generation.\"\"\"\n",
    "    code_gen_state.iteration += 1\n",
    "\n",
    "\n",
    "def codegen_update_code(code_gen_state: CodeGenState, code_output: Any) -> None:\n",
    "    \"\"\"Update the current code and save previous code snapshot.\"\"\"\n",
    "    if code_gen_state.current_code:\n",
    "        code_gen_state.previous_code = code_gen_state.current_code.code\n",
    "    code_gen_state.current_code = code_output\n",
    "\n",
    "\n",
    "def codegen_mark_complete(code_gen_state: CodeGenState) -> None:\n",
    "    \"\"\"Mark the code generation process as complete.\"\"\"\n",
    "    code_gen_state.process_completed = True\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentSafetyResult:\n",
    "    \"\"\"Results from content safety analysis\"\"\"\n",
    "    is_appropriate: bool = True\n",
    "    reason: str = \"Content is appropriate\"\n",
    "\n",
    "\n",
    "    category: str = \"safe\"\n",
    "@dataclass\n",
    "class MainState(): \n",
    "    \"\"\"\n",
    "    Main state object that provides a clean interface for the application.\n",
    "    This wraps the code generation state and tracks usage metrics.\n",
    "    Extends AgentState to support LangGraph's InjectedState functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages] = field(default_factory=list)\n",
    "\n",
    "    # Configuration\n",
    "    use_persistent_memory: bool = True\n",
    "    model_name: str = \"gpt-3.5-turbo\"\n",
    "    enable_content_safety: bool = True\n",
    "    \n",
    "    routeName: Optional[str] = field(default=None)\n",
    "    \n",
    "    # Content safety state\n",
    "    content_safety_result: Optional[ContentSafetyResult] = field(default=None)\n",
    "    \n",
    "    # Code generation state\n",
    "    code_gen_state: Optional[CodeGenState] = field(default=None)\n",
    "    \n",
    "    # Token usage tracking (new)\n",
    "    requests: int = 0\n",
    "    input_tokens: int = 0\n",
    "    output_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "    input_token_price_per_million: float = 2.5\n",
    "    output_token_price_per_million: float = 10\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize code_gen_state with default values if not provided\"\"\"\n",
    "        if self.code_gen_state is None:\n",
    "            self.code_gen_state = CodeGenState()\n",
    "\n",
    "\n",
    "# Standalone token usage tracking functions\n",
    "def update_usage(state: MainState, usage_dict: Dict[str, int]):\n",
    "    \"\"\"Update token usage from a usage dictionary\"\"\"\n",
    "    state.requests += usage_dict.get(\"requests\", 0)\n",
    "    state.input_tokens += usage_dict.get(\"input_tokens\", 0)\n",
    "    state.output_tokens += usage_dict.get(\"output_tokens\", 0)\n",
    "    state.total_tokens += usage_dict.get(\"total_tokens\", 0)\n",
    "\n",
    "\n",
    "def calculate_cost(state: MainState) -> float:\n",
    "    \"\"\"Calculate the total cost based on token usage\"\"\"\n",
    "    input_cost = (state.input_tokens / 1_000_000) * state.input_token_price_per_million\n",
    "    output_cost = (state.output_tokens / 1_000_000) * state.output_token_price_per_million\n",
    "    return input_cost + output_cost\n",
    "\n",
    "\n",
    "def get_usage_summary(state: MainState) -> str:\n",
    "    \"\"\"Get a formatted summary of token usage and cost\"\"\"\n",
    "    total_cost = calculate_cost(state)\n",
    "    return (\n",
    "        f\"üìä Token Usage:\\n\"\n",
    "        f\"  ‚Ä¢ Requests: {state.requests}\\n\"\n",
    "        f\"  ‚Ä¢ Input tokens: {state.input_tokens:,}\\n\"\n",
    "        f\"  ‚Ä¢ Output tokens: {state.output_tokens:,}\\n\"\n",
    "        f\"  ‚Ä¢ Total tokens: {state.total_tokens:,}\\n\"\n",
    "        f\"  ‚Ä¢ Total cost: ${total_cost:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d39ea0",
   "metadata": {},
   "source": [
    "### CIF/tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1bd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tools module for the chat system using LangChain tool decorators\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, List\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from langchain_core.tools import tool\n",
    "# from state import MainState, update_usage\n",
    "\n",
    "# MCP Integration imports\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from mcp.client.stdio import StdioServerParameters, stdio_client\n",
    "from mcp.client.session import ClientSession\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def add_numbers(a: float, b: float, state: MainState = None) -> str:\n",
    "    \"\"\"Add two numbers together and return the result.\n",
    "    \n",
    "    Args:\n",
    "        a: First number to add\n",
    "        b: Second number to add\n",
    "        state: The current agent state (automatically injected)\n",
    "        \n",
    "    Returns:\n",
    "        String describing the result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = a + b\n",
    "        return f\"The sum of {a} and {b} is {result}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in sum calculation: {e}\")\n",
    "        return f\"Error calculating sum: {str(e)}\"\n",
    "\n",
    "\n",
    "def multiply_numbers(a: float, b: float, state: MainState = None) -> str:\n",
    "    \"\"\"Multiply two numbers together and return the product.\n",
    "    \n",
    "    Args:\n",
    "        a: First number to multiply\n",
    "        b: Second number to multiply\n",
    "        state: The current agent state (automatically injected)\n",
    "        \n",
    "    Returns:\n",
    "        String describing the result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = a * b\n",
    "        return f\"The product of {a} and {b} is {result}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in multiplication: {e}\")\n",
    "        return f\"Error calculating product: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_code(prompt: str, state: MainState) -> str:\n",
    "    \"\"\"Generate complete code implementations with tests using a multi-agent system.\n",
    "    \n",
    "    This tool creates production-ready Python code with proper structure and comprehensive test coverage.\n",
    "    It iteratively improves the code until all tests pass.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Description of the code to generate\n",
    "        state: The current agent state (automatically injected)\n",
    "        \n",
    "    Returns:\n",
    "        String describing the generation result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not prompt:\n",
    "            return \"‚ùå Error: 'prompt' parameter is required and cannot be empty\"\n",
    "        \n",
    "        # Clear the app folder before code generation (user preference)\n",
    "        app_folder = Path(\"app\")\n",
    "        if app_folder.exists():\n",
    "            try:\n",
    "                shutil.rmtree(app_folder)\n",
    "                logger.info(\"Cleared existing app folder before code generation\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not clear app folder: {e}\")\n",
    "        \n",
    "        from code_generator_multiagent import CodeOrchestratorAgent\n",
    "        from state import CodeGenState\n",
    "        \n",
    "        # Ensure code_gen_state is initialized (since __post_init__ doesn't work with TypedDict)\n",
    "        if state.code_gen_state is None:\n",
    "            state.code_gen_state = CodeGenState()\n",
    "        \n",
    "        # Set the prompt in the code generation state\n",
    "        state.code_gen_state.user_prompt_for_app = prompt\n",
    "        \n",
    "        # Create and run orchestrator with state (use the main state's LLM if available)\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        llm_client = init_chat_model(state.model_name, model_provider=\"openai\")\n",
    "        orchestrator = CodeOrchestratorAgent(llm_client=llm_client)\n",
    "        final_state = orchestrator.run(state)\n",
    "        \n",
    "        # Extract token usage from code generation and update main state\n",
    "        if hasattr(final_state, '_code_gen_usage') and final_state._code_gen_usage:\n",
    "            update_usage(state, final_state._code_gen_usage)\n",
    "            logger.info(f\"üî¢ Code generation consumed {final_state._code_gen_usage['total_tokens']} tokens \"\n",
    "                       f\"across {final_state._code_gen_usage['requests']} LLM requests\")\n",
    "        \n",
    "        package_name = final_state.code_gen_state.package_name\n",
    "        \n",
    "        if final_state.code_gen_state and final_state.code_gen_state.process_completed:\n",
    "            test_info = \"\"\n",
    "            if final_state.code_gen_state.test_results:\n",
    "                test_results = final_state.code_gen_state.test_results\n",
    "                test_info = f\" Tests: {test_results.passed} passed, {test_results.failed} failed.\"\n",
    "            \n",
    "            return (\n",
    "                f\"‚úÖ Code generated successfully in {final_state.code_gen_state.iteration} iteration(s)!{test_info}\\n\"\n",
    "                f\"Check the '{package_name}' folder for the generated code.\\n\"\n",
    "                f\"Files created: main.py and test_main.py\"\n",
    "            )\n",
    "        else:\n",
    "            iterations = final_state.code_gen_state.iteration if final_state.code_gen_state else \"unknown\"\n",
    "            return (\n",
    "                f\"‚ö†Ô∏è Code generation incomplete after {iterations} iteration(s).\\n\"\n",
    "                f\"Some tests may be failing. Check the '{package_name}' folder for partial results.\"\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Code generation error: {e}\", exc_info=True)\n",
    "        return f\"‚ùå Code generation failed: {str(e)}\"\n",
    "\n",
    "\n",
    "async def load_mcp_weather_tools() -> List[Any]:\n",
    "    \"\"\"Load MCP weather tools using langchain_mcp_adapters following the provided example.\n",
    "    \n",
    "    Returns:\n",
    "        List of MCP weather tools wrapped as LangChain tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if API key is set\n",
    "        if not os.environ.get(\"ACCUWEATHER_API_KEY\"):\n",
    "            logger.warning(\"ACCUWEATHER_API_KEY not set in environment variables. Weather tools may not work properly.\")\n",
    "        \n",
    "        # Configure the MCP server parameters for weather service with environment\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"npx\",\n",
    "            args=[\"-y\", \"@timlukahorstmann/mcp-weather\"],\n",
    "            env=os.environ.copy()  # Pass current environment including API key\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Connecting to MCP weather server...\")\n",
    "        \n",
    "        # Connect and load tools using the langchain MCP adapter (following the example)\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                await session.initialize()\n",
    "                mcp_tools = await load_mcp_tools(session)\n",
    "                \n",
    "                logger.info(f\"Successfully loaded {len(mcp_tools)} MCP weather tools\")\n",
    "                return mcp_tools\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load MCP weather tools: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _run_async_in_thread(async_func, *args, **kwargs):\n",
    "    \"\"\"Helper to run async function in new thread with new event loop.\"\"\"\n",
    "    import concurrent.futures\n",
    "    \n",
    "    def run_in_loop():\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            return loop.run_until_complete(async_func(*args, **kwargs))\n",
    "        finally:\n",
    "            loop.close()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        return executor.submit(run_in_loop).result(timeout=30)\n",
    "\n",
    "\n",
    "async def _call_mcp_tool(tool_name: str, kwargs: dict) -> str:\n",
    "    \"\"\"Call MCP tool with fresh connection.\"\"\"\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"npx\", \n",
    "        args=[\"-y\", \"@timlukahorstmann/mcp-weather\"],\n",
    "        env=os.environ.copy()\n",
    "    )\n",
    "    \n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await load_mcp_tools(session)\n",
    "            \n",
    "            for tool in tools:\n",
    "                if tool.name == tool_name:\n",
    "                    return await tool.ainvoke(kwargs)\n",
    "            \n",
    "            return f\"Tool {tool_name} not found\"\n",
    "\n",
    "\n",
    "def create_sync_tool_wrapper(async_tool):\n",
    "    \"\"\"Create a synchronous wrapper for an async MCP tool.\"\"\"\n",
    "    from langchain_core.tools import BaseTool\n",
    "    from typing import Type, Any, Optional\n",
    "    \n",
    "    class SyncMCPTool(BaseTool):\n",
    "        name: str = async_tool.name\n",
    "        description: str = async_tool.description\n",
    "        args_schema: Optional[Type[Any]] = async_tool.args_schema\n",
    "        \n",
    "        def _run(self, **kwargs) -> str:\n",
    "            if not os.environ.get(\"ACCUWEATHER_API_KEY\"):\n",
    "                return \"‚ùå ACCUWEATHER_API_KEY not set in environment variables.\"\n",
    "            \n",
    "            try:\n",
    "                return _run_async_in_thread(_call_mcp_tool, async_tool.name, kwargs)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calling MCP tool {self.name}: {e}\")\n",
    "                return f\"‚ùå Weather service error: {str(e)}\"\n",
    "    \n",
    "    return SyncMCPTool()\n",
    "\n",
    "\n",
    "def get_mcp_weather_tools() -> List[Any]:\n",
    "    \"\"\"Get MCP weather tools as synchronous wrappers.\"\"\"\n",
    "    try:\n",
    "        async_tools = _run_async_in_thread(load_mcp_weather_tools)\n",
    "        sync_tools = [create_sync_tool_wrapper(tool) for tool in async_tools]\n",
    "        \n",
    "        for tool in sync_tools:\n",
    "            logger.info(f\"Created sync wrapper for: {tool.name}\")\n",
    "        \n",
    "        return sync_tools\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading MCP weather tools: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_all_tools(state: MainState = None) -> List[Any]:\n",
    "    \"\"\"Get all available tools for the agent.\n",
    "    \n",
    "    Args:\n",
    "        state: Optional MainState to bind to tools that need it\n",
    "    \n",
    "    Returns:\n",
    "        List of function tools that can be used with LangGraph agents\n",
    "    \"\"\"\n",
    "    \n",
    "    @tool\n",
    "    def add_numbers_bound(a: float, b: float) -> str:\n",
    "        \"\"\"Add two numbers together and return the result.\n",
    "        \n",
    "        Args:\n",
    "            a: First number to add\n",
    "            b: Second number to add\n",
    "            \n",
    "        Returns:\n",
    "            String describing the result\n",
    "        \"\"\"\n",
    "        return add_numbers(a, b, state)\n",
    "    \n",
    "    @tool\n",
    "    def multiply_numbers_bound(a: float, b: float) -> str:\n",
    "        \"\"\"Multiply two numbers together and return the product.\n",
    "        \n",
    "        Args:\n",
    "            a: First number to multiply\n",
    "            b: Second number to multiply\n",
    "            \n",
    "        Returns:\n",
    "            String describing the result\n",
    "        \"\"\"\n",
    "        return multiply_numbers(a, b, state)\n",
    "    \n",
    "    @tool\n",
    "    def generate_code_bound(prompt: str) -> str:\n",
    "        \"\"\"Generate complete code implementations with tests using a multi-agent system.\n",
    "        \n",
    "        This tool creates production-ready Python code with proper structure and comprehensive test coverage.\n",
    "        It iteratively improves the code until all tests pass.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Description of the code to generate\n",
    "            \n",
    "        Returns:\n",
    "            String describing the generation result\n",
    "        \"\"\"\n",
    "        return generate_code(prompt, state)\n",
    "    \n",
    "    # Start with core tools\n",
    "    tools = [add_numbers_bound, multiply_numbers_bound, generate_code_bound]\n",
    "    \n",
    "    # Add MCP weather tools\n",
    "    try:\n",
    "        mcp_weather_tools = get_mcp_weather_tools()\n",
    "        if mcp_weather_tools:\n",
    "            tools.extend(mcp_weather_tools)\n",
    "            logger.info(f\"Added {len(mcp_weather_tools)} MCP weather tools to available tools\")\n",
    "        else:\n",
    "            logger.info(\"No MCP weather tools available\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to add MCP weather tools: {e}\")\n",
    "    \n",
    "    return tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43bb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99512536",
   "metadata": {},
   "source": [
    "### CIF/prompts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfac4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CODER_PROMPT_TEMPLATE = \"\"\"You are an expert Python developer. Your task is to create a complete Python application based on the requirements below.\n",
    "\n",
    "## Response Format\n",
    "You must respond with a JSON object containing:\n",
    "1. \"code\": A string containing the complete Python code for the application (WITH FULL TYPE HINTS)\n",
    "2. \"explanation\": A brief explanation of what you created\n",
    "\n",
    "**CRITICAL**: The code MUST include type hints for ALL function parameters and return values.\n",
    "\n",
    "Example response format:\n",
    "{{\n",
    "    \"code\": \"import sys\\\\nimport os\\\\nfrom typing import Optional, List\\\\n\\\\ndef main() -> int:\\\\n    print('Hello World')\\\\n    return 0\\\\n\\\\nif __name__ == '__main__':\\\\n    sys.exit(main())\",\n",
    "    \"explanation\": \"Created a Python application with full type hints...\"\n",
    "}}\n",
    "\n",
    "## Project Requirements:\n",
    "{user_requirements}\n",
    "\n",
    "\n",
    "## Current Iteration:\n",
    "Iteration #{iteration}\n",
    "\n",
    "{previous_work}\n",
    "\n",
    "## Code Generation Guidelines:\n",
    "\n",
    "### Architecture & Design:\n",
    "- Use clear separation of concerns with modular architecture\n",
    "- Implement proper error handling with try/except blocks\n",
    "- Use dependency injection where appropriate\n",
    "- Follow SOLID principles\n",
    "- Create reusable components and utilities\n",
    "- **DO NOT USE ARGPARSE**: Functions should accept parameters directly with sensible defaults\n",
    "\n",
    "### Code Quality Standards:\n",
    "- Write Python 3.8+ compatible code using modern Python features\n",
    "- Follow PEP 8 style guide strictly\n",
    "- **MANDATORY**: Use type hints for ALL function parameters and return values\n",
    "  * Example: def process_data(input_text: str, max_length: int = 100) -> Dict[str, Any]:\n",
    "  * Import typing module: from typing import List, Dict, Optional, Union, Any, Tuple\n",
    "  * Use Optional[T] for nullable types\n",
    "  * Use Union[T1, T2] for multiple possible types\n",
    "- Implement comprehensive error messages with context\n",
    "- Add logging statements for debugging and monitoring\n",
    "- Use descriptive variable and function names\n",
    "\n",
    "### Documentation:\n",
    "- Add comprehensive docstrings (Google style) for all classes and functions\n",
    "- Include usage examples in docstrings\n",
    "- Add inline comments for complex logic\n",
    "\n",
    "### Performance & Security:\n",
    "- Optimize for readability first, then performance\n",
    "- Validate all inputs and sanitize user data\n",
    "- Use context managers for resource management\n",
    "- Implement proper connection pooling for external services\n",
    "- Avoid hardcoded secrets or credentials\n",
    "\n",
    "### Code Structure:\n",
    "- Generate a single, complete Python script that can be run directly\n",
    "- Include all necessary imports at the top (including typing module)\n",
    "- **ALWAYS** start with: from typing import List, Dict, Optional, Union, Any, Tuple, etc.\n",
    "- Organize code with clear functions and classes\n",
    "- Separate concerns into different functions\n",
    "- Use meaningful function and variable names with proper type annotations\n",
    "- **IMPORTANT**: Create a main() function that accepts parameters with default values instead of using command-line arguments\n",
    "  * Example: def main(input_data: str = \"default value\", max_items: int = 10) -> int:\n",
    "  * This makes the code easily testable without requiring command-line invocation\n",
    "  * All parameters should have sensible defaults when possible\n",
    "- Include if __name__ == \"__main__\" block that calls main() with default values\n",
    "- The code should be self-contained and runnable\n",
    "- Every function MUST have type hints for parameters and return values\n",
    "- **DO NOT USE argparse or sys.argv**: All input should come through function parameters\n",
    "\n",
    "### Testing Considerations:\n",
    "- Structure code to be easily testable\n",
    "- Main logic should be in functions that can be imported and tested\n",
    "- Avoid global state or side effects in core logic functions\n",
    "- Return values instead of printing directly when possible (except in main())\n",
    "- Dont create and test code in main()\n",
    "\n",
    "### Dependencies:\n",
    "- Keep external dependencies minimal\n",
    "- Use standard library where possible\n",
    "- If external packages are needed, mention them in comments\n",
    "- **DO NOT import argparse**\n",
    "\n",
    "{constraints_section}\n",
    "\n",
    "{review_feedback}\n",
    "\n",
    "Please generate the complete, production-ready Python code as a single script following all the guidelines above.\n",
    "\"\"\"\n",
    "\n",
    "TESTER_PROMPT_TEMPLATE = \"\"\"You are an expert Python testing specialist. Your task is to create comprehensive unit tests for the provided Python code.\n",
    "\n",
    "## Response Format\n",
    "You must respond with a JSON object containing:\n",
    "1. \"test_code\": A string containing the complete test file code\n",
    "2. \"explanation\": A brief explanation of the test coverage and strategy\n",
    "\n",
    "Example response format:\n",
    "{{\n",
    "    \"test_code\": \"import unittest\\\\nimport sys\\\\nsys.path.append('..')\\\\nfrom main import *\\\\n\\\\nclass TestMain(unittest.TestCase):\\\\n    def test_example(self):\\\\n        # Test implementation\\\\n        pass\\\\n\\\\nif __name__ == '__main__':\\\\n    unittest.main()\",\n",
    "    \"explanation\": \"Created comprehensive unit tests covering all main functions...\"\n",
    "}}\n",
    "\n",
    "## Code to Test:\n",
    "{code_to_test}\n",
    "\n",
    "## Maximum Number of Tests: {max_test}\n",
    "\n",
    "## Test Generation Guidelines:\n",
    "\n",
    "### Test Structure:\n",
    "- Create a complete test file using unittest framework\n",
    "- Import the main module: `import sys; sys.path.append('..'); from main import *`\n",
    "- Create TestMain class inheriting from unittest.TestCase\n",
    "- Include if __name__ == '__main__': unittest.main() block\n",
    "\n",
    "### Test Coverage:\n",
    "- Test all public functions in the main module (within the test limit)\n",
    "- Include positive test cases (expected behavior)\n",
    "- Include negative test cases (error handling)\n",
    "- Test edge cases and boundary conditions\n",
    "- Test different input types and values\n",
    "- Verify return values and types match expectations\n",
    "- Prioritize the most important test cases if the limit is reached\n",
    "\n",
    "### Test Quality:\n",
    "- Use descriptive test method names (test_function_name_scenario)\n",
    "- Add docstrings explaining what each test validates\n",
    "- Use unittest assertions (assertEqual, assertRaises, assertTrue, etc.)\n",
    "- Mock external dependencies if needed\n",
    "- Test both success and failure paths\n",
    "\n",
    "### Error Testing:\n",
    "- Test invalid inputs raise appropriate exceptions\n",
    "- Verify error messages are meaningful\n",
    "- Test type validation if present\n",
    "- Test boundary conditions (empty strings, None, negative numbers, etc.)\n",
    "\n",
    "### Test Organization:\n",
    "- Group related tests logically\n",
    "- Use setUp/tearDown if needed for test fixtures\n",
    "- Keep tests independent (no test should depend on another)\n",
    "- Each test should focus on one specific behavior\n",
    "\n",
    "### Imports and Setup:\n",
    "- Import unittest and any other testing utilities needed\n",
    "- Import the module under test properly\n",
    "- Add sys.path.append('..') to import from parent directory\n",
    "- Import any additional libraries used in the main code\n",
    "\n",
    "Generate comprehensive, well-structured unit tests that thoroughly validate the functionality of the provided code.\n",
    "\"\"\"\n",
    "\n",
    "REVIEWER_PROMPT_TEMPLATE = \"\"\"You are an expert code reviewer and quality assurance specialist. Your task is to analyze the generated code and test results to provide actionable feedback.\n",
    "\n",
    "## Response Format\n",
    "You must respond with a JSON object containing:\n",
    "1. \"review_notes\": An array of specific, actionable feedback items\n",
    "2. \"analysis\": A brief overall analysis of the code quality and test results\n",
    "3. \"recommendation\": Either \"approve\" (if all tests pass) or \"revise\" (if improvements needed)\n",
    "\n",
    "Example response format:\n",
    "{{\n",
    "    \"review_notes\": [\n",
    "        \"Fix the edge case handling in slugify function for empty strings\",\n",
    "        \"Add input validation for max_length parameter to ensure positive integers\",\n",
    "        \"Improve error messages to be more descriptive\"\n",
    "    ],\n",
    "    \"analysis\": \"The code has good structure but fails several edge case tests...\",\n",
    "    \"recommendation\": \"revise\"\n",
    "}}\n",
    "\n",
    "## Code to Review:\n",
    "{code}\n",
    "\n",
    "## Test Results:\n",
    "- Tests Passed: {passed}\n",
    "- Tests Failed: {failed}\n",
    "- Test Explanation: {test_explanation}\n",
    "\n",
    "## Failed Test Details:\n",
    "{failure_details}\n",
    "\n",
    "## Review Guidelines:\n",
    "\n",
    "### If All Tests Pass (failed == 0):\n",
    "- Acknowledge successful implementation\n",
    "- Suggest optional improvements for code quality\n",
    "- Recommend approval to complete the process\n",
    "\n",
    "### If Tests Fail (failed > 0):\n",
    "- Analyze each test failure carefully\n",
    "- Identify the root cause of failures\n",
    "- Provide specific fixes for each failure\n",
    "- Consider edge cases that might be missing\n",
    "- Suggest improvements to error handling\n",
    "\n",
    "### Code Quality Assessment:\n",
    "- Check for proper type hints on all functions\n",
    "- Verify error handling is comprehensive\n",
    "- Ensure code follows PEP 8 style guidelines\n",
    "- Check for proper input validation\n",
    "- Verify Unicode normalization is correctly implemented\n",
    "- Ensure max-length handling works correctly\n",
    "- Check for proper logging/documentation\n",
    "\n",
    "### Focus Areas for Failed Tests:\n",
    "- Input validation (empty strings, None, invalid types)\n",
    "- Edge cases (special characters only, very long strings)\n",
    "- Unicode handling (accented characters, non-ASCII)\n",
    "- Max-length boundaries (cutting at word boundaries)\n",
    "- Error messages and exceptions\n",
    "\n",
    "### Actionable Feedback:\n",
    "- Be specific about what needs to be fixed\n",
    "- Reference the exact function or line that needs changes\n",
    "- Provide clear guidance on how to fix each issue\n",
    "- Prioritize critical failures over minor improvements\n",
    "\n",
    "Generate a comprehensive review with specific, actionable feedback to guide the next iteration.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Agent Routing Prompt\n",
    "AGENT_ROUTING_PROMPT = \"\"\"You are a routing system that determines which agent should handle a user request.\n",
    "\n",
    "Analyze the conversation history and the latest user request to determine the appropriate agent.\n",
    "\n",
    "**legal_expert** - If the request is about:\n",
    "- Law, legal matters, regulations\n",
    "- Rights, contracts, legal procedures\n",
    "- Court cases, legal precedents\n",
    "- Legal advice or legal systems\n",
    "- Compliance, legal documents\n",
    "\n",
    "**general_agent** - If the request is about:\n",
    "- EVERYTHING that is not law-related\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Respond with ONLY the agent name: either \"legal_expert\" or \"general_agent\"\n",
    "- No explanations, no additional text\n",
    "- No quotes, no punctuation\n",
    "- Just the agent name as a single word\n",
    "\n",
    "Conversation history and current request:\n",
    "{messages}\n",
    "\n",
    "Agent to route to:\"\"\"\n",
    "\n",
    "\n",
    "# Legal Expert Prompt\n",
    "LEGAL_EXPERT_PROMPT = \"\"\"You are a knowledgeable legal expert specializing in law and legal matters.\n",
    "\n",
    "Your expertise covers:\n",
    "- Constitutional law and civil rights\n",
    "- Criminal law and procedures\n",
    "- Contract law and agreements\n",
    "- Corporate and business law\n",
    "- Intellectual property (patents, trademarks, copyright)\n",
    "- International law and treaties\n",
    "- Legal procedures and court systems\n",
    "- Regulatory compliance\n",
    "- Legal documentation and terminology\n",
    "- Rights and obligations under various jurisdictions\n",
    "\n",
    "Provide accurate, informative responses about legal topics while being clear that you're providing educational information, not formal legal advice.\n",
    "Be thorough in explaining legal concepts, cite relevant laws or precedents when applicable, and help users understand complex legal matters.\n",
    "\n",
    "IMPORTANT: Always clarify that your responses are for informational purposes only and that users should consult with a qualified attorney for specific legal advice.\n",
    "\n",
    "Conversation history:\n",
    "{messages}\n",
    "\n",
    "Legal response:\"\"\"\n",
    "\n",
    "\n",
    "# General Agent Prompt\n",
    "GENERAL_AGENT_PROMPT = \"\"\"You are a helpful AI assistant with access to various tools and MCP servers.\n",
    "\n",
    "You can handle:\n",
    "- Programming and code generation\n",
    "- File operations and system commands\n",
    "- General knowledge questions (history, science, arts, culture)\n",
    "- Technical debugging and analysis\n",
    "- Calculations and data processing\n",
    "- Any task that is NOT specifically about law or legal matters\n",
    "\n",
    "Be helpful, concise, and friendly. Use your tools effectively to assist users with their requests.\n",
    "\n",
    "Conversation history:\n",
    "{messages}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "CONTENT_SAFETY_PROMPT = \"\"\"You are a content safety system. Your job is to analyze user requests and determine if they contain inappropriate content.\n",
    "\n",
    "Check for the following categories of inappropriate content:\n",
    "\n",
    "1. **Illegal Activities**: \n",
    "   - Requests for help with illegal activities (fraud, hacking, theft, drug manufacturing, etc.)\n",
    "   - Instructions on how to break laws or evade law enforcement\n",
    "   - Creation of fake documents or identities\n",
    "   - Money laundering or financial crimes\n",
    "   \n",
    "2. **Violence and Harm**: \n",
    "   - Requests involving violence, self-harm, or harm to others\n",
    "   - Instructions for creating weapons or dangerous devices\n",
    "   - Threatening or harassing content\n",
    "   - Content promoting dangerous behaviors\n",
    "   \n",
    "3. **Hate Speech and Discrimination**: \n",
    "   - Content that promotes hatred or discrimination based on race, ethnicity, religion, gender, sexual orientation, disability, etc.\n",
    "   - Dehumanizing language or stereotypes\n",
    "   \n",
    "4. **Sexual or Adult Content**: \n",
    "   - Sexually explicit content or requests\n",
    "   - Content involving minors in inappropriate contexts\n",
    "   \n",
    "5. **Privacy Violations**: \n",
    "   - Requests for personal information about individuals\n",
    "   - Doxxing or exposure of private information\n",
    "   - Stalking or surveillance instructions\n",
    "   \n",
    "6. **Misinformation and Manipulation**: afe\n",
    "   - Requests to create misleading or false information\n",
    "   - Conspiracy theories intended to harm\n",
    "   - Impersonation requests\n",
    "   \n",
    "7. **Academic Dishonesty**: \n",
    "   - Direct requests to complete homework, essays, or exams\n",
    "   - Plagiarism assistance\n",
    "   \n",
    "8. **Unethical AI Use**: \n",
    "   - Attempts to jailbreak or bypass AI safety measures\n",
    "   - Requests to role-play as harmful entities\n",
    "   - Manipulative prompt engineering to extract harmful content\n",
    "\n",
    "Respond ONLY with a valid JSON object (no other text, no markdown formatting) containing exactly these fields:\n",
    "{\n",
    "  \"is_inappropriate\": true/false,\n",
    "  \"reason\": \"brief explanation\",\n",
    "  \"category\": \"category_name\"\n",
    "}\n",
    "\n",
    "- \"is_inappropriate\": true if ANY of the above categories are detected, false otherwise\n",
    "- \"reason\": A brief explanation of why the content is inappropriate (or \"Content is appropriate\" if safe)\n",
    "- \"category\": The primary category of violation (e.g., \"illegal\", \"violence\", \"hate_speech\", \"sexual_content\", \"privacy_violation\", \"misinformation\", \"academic_dishonesty\", \"unethical_ai_use\", \"safe\")\n",
    "\n",
    "Be cautious but not overly restrictive. Educational discussions about these topics in appropriate contexts may be acceptable.\n",
    "Focus on actual harmful intent rather than legitimate educational or safety discussions.\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON object, no additional text or formatting.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecec8e",
   "metadata": {},
   "source": [
    "### CIF/agents.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6364e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "# from prompts import AGENT_ROUTING_PROMPT, LEGAL_EXPERT_PROMPT, GENERAL_AGENT_PROMPT, CONTENT_SAFETY_PROMPT\n",
    "import logging\n",
    "import json\n",
    "# from state import MainState, ContentSafetyResult\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage, AIMessage    \n",
    "# from tools import get_all_tools\n",
    "from langchain_classic.agents import create_tool_calling_agent ,AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "        \n",
    "\n",
    "class ResponseFormatter(BaseModel):\n",
    "    routing_decision: str\n",
    "\n",
    "class ContentSafetyAgent:\n",
    "    def __init__(self, orchestrator_agent):\n",
    "        self.orchestrator_agent = orchestrator_agent\n",
    "        self.llm = self.orchestrator_agent.llm\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"ContentSafetyAgent initialized.\")\n",
    "\n",
    "    def run(self, state: MainState, config: RunnableConfig):\n",
    "        \"\"\"\n",
    "        Analyze user input for content safety violations.\n",
    "        Returns updated state with safety analysis results.\n",
    "        \"\"\"\n",
    "        # Extract the latest user message\n",
    "        user_input = \"\"\n",
    "        messages = state.get(\"messages\", []) if isinstance(state, dict) else state.messages\n",
    "        if messages:\n",
    "            for message in reversed(messages):\n",
    "                if isinstance(message, HumanMessage):\n",
    "                    user_input = message.content\n",
    "                    break\n",
    "\n",
    "        # Skip safety check if content safety is disabled\n",
    "        enable_content_safety = state.get(\"enable_content_safety\", True) if isinstance(state, dict) else state.enable_content_safety\n",
    "        if not enable_content_safety:\n",
    "            self.logger.info(\"Content safety check disabled, proceeding without analysis.\")\n",
    "            return {\"content_safety_result\": ContentSafetyResult()}\n",
    "\n",
    "        try:\n",
    "            # Create safety analysis prompt\n",
    "            safety_prompt = f\"{CONTENT_SAFETY_PROMPT}\\n\\nUser message to analyze: {user_input}\"\n",
    "            \n",
    "            # Get safety assessment from LLM\n",
    "            response = self.llm.invoke(safety_prompt)\n",
    "            response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            safety_data = json.loads(response_content.strip())\n",
    "            \n",
    "            # Create safety result\n",
    "            safety_result = ContentSafetyResult(\n",
    "                is_appropriate=not safety_data.get(\"is_inappropriate\", False),\n",
    "                reason=safety_data.get(\"reason\", \"Content analysis completed\"),\n",
    "                category=safety_data.get(\"category\", \"safe\")\n",
    "            )\n",
    "            \n",
    "            # Log safety analysis results\n",
    "            if not safety_result.is_appropriate:\n",
    "                self.logger.warning(\n",
    "                    f\"üõ°Ô∏è CONTENT SAFETY VIOLATION DETECTED - \"\n",
    "                    f\"reason: '{safety_result.reason}', \"\n",
    "                    f\"category: '{safety_result.category}'\"\n",
    "                )\n",
    "                self.logger.warning(f\"üõ°Ô∏è Flagged content: '{user_input[:100]}...' (truncated)\")\n",
    "            else:\n",
    "                self.logger.info(\"üõ°Ô∏è Content safety check passed - content is appropriate\")\n",
    "            \n",
    "            return {\"content_safety_result\": safety_result}\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.error(f\"Failed to parse content safety JSON response: {e}\")\n",
    "            # Default to safe on parsing error to avoid blocking legitimate content\n",
    "            return {\"content_safety_result\": ContentSafetyResult()}\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Content safety analysis error: {e}\")\n",
    "            # Default to safe on error to avoid blocking legitimate content\n",
    "            return {\"content_safety_result\": ContentSafetyResult()}\n",
    "\n",
    "class RoutingAgent:\n",
    "    def __init__(self, orchestrator_agent):\n",
    "        self.orchestrator_agent = orchestrator_agent\n",
    "        self.structured_llm = self.orchestrator_agent.llm.with_structured_output(ResponseFormatter) \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"RoutingAgent initialized.\")\n",
    "\n",
    "    def run(self, state: MainState, config: RunnableConfig):\n",
    "        messages = state.get(\"messages\", []) if isinstance(state, dict) else state.messages\n",
    "        \n",
    "        # Format messages for the prompt\n",
    "        messages_text = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                messages_text += f\"User: {message.content}\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                messages_text += f\"Assistant: {message.content}\\n\"\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"messages\"],\n",
    "            template=AGENT_ROUTING_PROMPT\n",
    "        )\n",
    "\n",
    "        formatted_prompt = prompt_template.format(messages=messages_text)\n",
    "        response = self.structured_llm.invoke(formatted_prompt)\n",
    "        response_dict = response.dict() \n",
    "        routeName = response_dict.get(\"routing_decision\", \"\")\n",
    "\n",
    "        return {\"routeName\": routeName}\n",
    "\n",
    "\n",
    "class LegalExpertAgent:\n",
    "    def __init__(self, orchestrator_agent):\n",
    "        self.orchestrator_agent = orchestrator_agent\n",
    "        self.llm = self.orchestrator_agent.llm\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"LegalExpertAgent initialized.\")\n",
    "\n",
    "    def run(self, state: MainState, config: RunnableConfig):\n",
    "        messages = state.get(\"messages\", []) if isinstance(state, dict) else state.messages\n",
    "        \n",
    "        messages_text = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                messages_text += f\"User: {message.content}\\n\"\n",
    "            elif isinstance(message, AIMessage):\n",
    "                messages_text += f\"Assistant: {message.content}\\n\"\n",
    "\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"messages\"],\n",
    "            template=LEGAL_EXPERT_PROMPT\n",
    "        )\n",
    "\n",
    "        formatted_prompt = prompt_template.format(messages=messages_text)\n",
    "        response = self.llm.invoke(formatted_prompt)\n",
    "        \n",
    "        response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        ai_message = AIMessage(content=response_content)\n",
    "        \n",
    "        return {\"messages\": [ai_message]}\n",
    "\n",
    "\n",
    "class GeneralAgent:\n",
    "    def __init__(self, orchestrator_agent):\n",
    "        self.orchestrator_agent = orchestrator_agent\n",
    "        self.llm = self.orchestrator_agent.llm\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Create a prompt that includes agent scratchpad for tool calling\n",
    "        self.tool_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", GENERAL_AGENT_PROMPT.replace(\"Conversation history:\\n{messages}\\n\\nResponse:\", \"\")),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\")\n",
    "        ])\n",
    "        \n",
    "        self.logger.info(\"GeneralAgent initialized - tools will be created per run with state context.\")\n",
    "\n",
    "    def run(self, state: MainState, config: RunnableConfig):\n",
    "        messages = state.get(\"messages\", []) if isinstance(state, dict) else state.messages\n",
    "        \n",
    "        # Format messages for context and get the latest user input\n",
    "        messages_text = \"\"\n",
    "        user_input = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                messages_text += f\"User: {message.content}\\n\"\n",
    "                user_input = message.content  # Keep the latest user input for the agent\n",
    "            elif isinstance(message, AIMessage):\n",
    "                messages_text += f\"Assistant: {message.content}\\n\"\n",
    "\n",
    "        try:\n",
    "            # Get tools with state access for this specific run\n",
    "            tools = get_all_tools(state)\n",
    "            \n",
    "            # Create the tool-calling agent with current state\n",
    "            agent = create_tool_calling_agent(\n",
    "                tools=tools,\n",
    "                llm=self.llm,\n",
    "                prompt=self.tool_prompt\n",
    "            )\n",
    "            \n",
    "            # Create the agent executor with current tools\n",
    "            agent_executor = AgentExecutor(\n",
    "                agent=agent,\n",
    "                tools=tools,\n",
    "                verbose=True,\n",
    "                handle_parsing_errors=True\n",
    "            )\n",
    "\n",
    "            # Include conversation context in the input\n",
    "            full_input = f\"Conversation history:\\n{messages_text}\\nCurrent request: {user_input}\"\n",
    "\n",
    "            response = agent_executor.invoke({\n",
    "                \"input\": full_input\n",
    "            })\n",
    "\n",
    "            response_content = response.get(\"output\", \"I couldn't process your request.\")\n",
    "\n",
    "            ai_message = AIMessage(content=response_content)\n",
    "            \n",
    "            return {\"messages\": [ai_message]}\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in GeneralAgent: {e}\")\n",
    "            error_message = f\"I encountered an error while processing your request: {str(e)}\"\n",
    "            ai_message = AIMessage(content=error_message)\n",
    "            \n",
    "            return {\"messages\": [ai_message]}\n",
    "\n",
    "\n",
    "class SafetyTerminationAgent:\n",
    "    def __init__(self, orchestrator_agent):\n",
    "        self.orchestrator_agent = orchestrator_agent\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"SafetyTerminationAgent initialized.\")\n",
    "\n",
    "    def run(self, state: MainState, config: RunnableConfig):\n",
    "        \"\"\"\n",
    "        Generate appropriate response for unsafe content and terminate the conversation.\n",
    "        \"\"\"\n",
    "        safety_result = state.get(\"content_safety_result\") if isinstance(state, dict) else state.content_safety_result\n",
    "        \n",
    "        if safety_result and not safety_result.is_appropriate:\n",
    "            # Create concise response with safety details\n",
    "            safety_response = (\n",
    "                f\"I can't assist with that request due to safety guidelines.\\n\\n\"\n",
    "                f\"Reason: {safety_result.reason}\\n\"\n",
    "                f\"Category: {safety_result.category}\\n\\n\"\n",
    "                f\"I'm happy to help with programming, general knowledge, legal education, \"\n",
    "                f\"calculations, or technical analysis instead!\"\n",
    "            )\n",
    "        else:\n",
    "            # Fallback response (shouldn't normally reach here)\n",
    "            safety_response = (\n",
    "                \"I'm unable to process your request at this time. \"\n",
    "                \"Please try rephrasing your question or ask about something else I can help with.\"\n",
    "            )\n",
    "        \n",
    "        # Create AI message with safety response\n",
    "        ai_message = AIMessage(content=safety_response)\n",
    "        \n",
    "        # Note: LangGraph checkpointer automatically handles message persistence\n",
    "        \n",
    "        return {\"messages\": [ai_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d12ff",
   "metadata": {},
   "source": [
    "### CIF/checkpointer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d35b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpointer Module for LangGraph Persistent Storage\n",
    "\n",
    "This module provides a Checkpointer class that initializes and manages\n",
    "persistent storage using SQLite through LangGraph's checkpointer system.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Checkpointer:\n",
    "    \"\"\"\n",
    "    Checkpointer class for managing persistent storage with SQLite.\n",
    "    \n",
    "    This class provides a simple interface to initialize and manage\n",
    "    persistent memory storage for LangGraph applications using SQLite checkpointer.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize(db_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize Checkpointer with SQLite checkpointer\n",
    "        \n",
    "        Args:\n",
    "            db_path (str, optional): Path to SQLite database file. \n",
    "                                   If None, uses './memory/langgraph_checkpoints.db'\n",
    "            \n",
    "        Returns:\n",
    "            SqliteSaver: Configured SQLite checkpointer instance\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set default database path if not provided\n",
    "            if db_path is None:\n",
    "                # Create a memory directory if it doesn't exist\n",
    "                memory_dir = Path(\"./memory\")\n",
    "                memory_dir.mkdir(exist_ok=True)\n",
    "                db_path = str(memory_dir / \"langgraph_checkpoints.db\")\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            db_dir = Path(db_path).parent\n",
    "            db_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            logger.info(f\"Initializing SQLite checkpointer at: {db_path}\")\n",
    "            \n",
    "            # Create SQLite connection with thread safety enabled\n",
    "            conn = sqlite3.connect(\n",
    "                db_path,\n",
    "                check_same_thread=False,  # Allow connection to be used across threads\n",
    "                timeout=30.0  # Add timeout to prevent deadlocks\n",
    "            )\n",
    "            \n",
    "            # Enable WAL mode for better concurrent access\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "            conn.commit()\n",
    "            \n",
    "            # Create the SQLite checkpointer with the connection\n",
    "            checkpointer = SqliteSaver(conn)\n",
    "            \n",
    "            logger.info(\"SQLite checkpointer initialized successfully\")\n",
    "            return checkpointer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize SQLite checkpointer: {e}\")\n",
    "            raise RuntimeError(f\"Database initialization failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508c712",
   "metadata": {},
   "source": [
    "### CIF/Orchestrotor_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebdaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "# from state import MainState, update_usage, calculate_cost, get_usage_summary\n",
    "# from agents import RoutingAgent, LegalExpertAgent, GeneralAgent, ContentSafetyAgent, SafetyTerminationAgent\n",
    "# from checkpointer import Checkpointer\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from core import logging_utils, utils\n",
    "\n",
    "logger = logging_utils.get_logger(__name__)\n",
    "\n",
    "class OrchestratorAgent(): \n",
    "    def __init__(self, session_id: str = \"default_session\"):\n",
    "        \n",
    "        self.session_id = session_id\n",
    "        self.state = MainState()\n",
    "        self.main_graph = StateGraph(MainState)\n",
    "        self.llm = init_chat_model(self.state.model_name, model_provider=\"openai\")\n",
    "        self.logger = logger\n",
    "        \n",
    "        # Initialize persistent checkpointer conditionally\n",
    "        if self.state.use_persistent_memory:\n",
    "            self.checkpointer = Checkpointer.initialize()\n",
    "            self.logger.info(\"Persistent memory enabled - checkpointer initialized\")\n",
    "        else:\n",
    "            self.checkpointer = None\n",
    "            self.logger.info(\"Persistent memory disabled - using session-only memory\")\n",
    "        \n",
    "        self.content_safety_agent = ContentSafetyAgent(self)\n",
    "        self.routing_agent = RoutingAgent(self)\n",
    "        self.legal_expert_agent = LegalExpertAgent(self)\n",
    "        self.general_agent = GeneralAgent(self)\n",
    "        self.safety_termination_agent = SafetyTerminationAgent(self)\n",
    "        memory_status = \"with persistent memory\" if self.state.use_persistent_memory else \"with session-only memory\"\n",
    "        self.logger.info(f\"OrchestratorAgent initialized {memory_status}.\")\n",
    "        \n",
    "        # All agents will be reinitialized at each node function call.\n",
    "        def _content_safety_agent_node(state: MainState, config: RunnableConfig):\n",
    "            return self.content_safety_agent.run(state, config)\n",
    "\n",
    "        def _routing_agent_node(state: MainState, config: RunnableConfig):\n",
    "            return self.routing_agent.run(state, config)\n",
    "\n",
    "        def _legal_expert_agent_node(state: MainState, config: RunnableConfig):\n",
    "            return self.legal_expert_agent.run(state, config)\n",
    "\n",
    "        def _general_agent_node(state: MainState, config: RunnableConfig):\n",
    "            return self.general_agent.run(state, config)\n",
    "\n",
    "        def _safety_termination_agent_node(state: MainState, config: RunnableConfig):\n",
    "            return self.safety_termination_agent.run(state, config)\n",
    "\n",
    "        self.main_graph.add_node(\"content_safety\", _content_safety_agent_node)\n",
    "        self.main_graph.add_node(\"routing_agent\", _routing_agent_node)\n",
    "        self.main_graph.add_node(\"legal_expert\", _legal_expert_agent_node) \n",
    "        self.main_graph.add_node(\"general_agent\", _general_agent_node)\n",
    "        self.main_graph.add_node(\"safety_termination\", _safety_termination_agent_node)\n",
    "\n",
    "        def safety_check_router(state: MainState):\n",
    "            \"\"\"Route based on content safety analysis\"\"\"\n",
    "            content_safety_result = state.get(\"content_safety_result\") if isinstance(state, dict) else state.content_safety_result\n",
    "            if content_safety_result and not content_safety_result.is_appropriate:\n",
    "                return \"safety_termination\"\n",
    "            else:\n",
    "                return \"routing_agent\"\n",
    "\n",
    "        def route_agent(state: MainState):\n",
    "            \"\"\"Route to appropriate agent based on routing decision\"\"\"\n",
    "            route_name = state.get(\"routeName\") if isinstance(state, dict) else state.routeName\n",
    "            if route_name == \"legal_expert\":\n",
    "                return \"legal_expert\"\n",
    "            else:\n",
    "                return \"general_agent\"\n",
    "\n",
    "        # Define graph edges with content safety first\n",
    "        self.main_graph.add_edge(START, \"content_safety\")\n",
    "        \n",
    "        self.main_graph.add_conditional_edges(\n",
    "            \"content_safety\",\n",
    "            safety_check_router,\n",
    "            {\n",
    "                \"safety_termination\": \"safety_termination\",\n",
    "                \"routing_agent\": \"routing_agent\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.main_graph.add_conditional_edges(\n",
    "            \"routing_agent\",\n",
    "            route_agent,\n",
    "            {\n",
    "                \"legal_expert\": \"legal_expert\",\n",
    "                \"general_agent\": \"general_agent\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # All paths lead to END\n",
    "        self.main_graph.add_edge(\"legal_expert\", END)\n",
    "        self.main_graph.add_edge(\"general_agent\", END)\n",
    "        self.main_graph.add_edge(\"safety_termination\", END)\n",
    "\n",
    "        # Compile graph with or without checkpointer based on configuration\n",
    "        if self.checkpointer is not None:\n",
    "            self.main_graph = self.main_graph.compile(checkpointer=self.checkpointer)\n",
    "        else:\n",
    "            self.main_graph = self.main_graph.compile()\n",
    "\n",
    "\n",
    "    def main_graph_invoke(self, user_input: str, session_id: str = None):\n",
    "        \"\"\"\n",
    "        Sends the user input into the LangGraph state machine and streams back the final answer.\n",
    "        Checkpointer automatically handles conversation persistence via MainState.messages.\n",
    "        Tracks token usage and cost for each interaction.\n",
    "        \"\"\"\n",
    "        # Use provided session_id or default to instance session_id\n",
    "        thread_id = session_id or self.session_id\n",
    "        \n",
    "\n",
    "        current_user_message = HumanMessage(content=user_input)\n",
    "        \n",
    "\n",
    "        input_data = {\n",
    "            \"messages\": [current_user_message]\n",
    "        }\n",
    "\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        # Initialize callback handler for token tracking\n",
    "        with get_openai_callback() as cb:\n",
    "            responses = self.main_graph.invoke(input_data, config)\n",
    "            \n",
    "            # Extract token usage from callback handler\n",
    "            usage_dict = {\n",
    "                \"requests\": cb.successful_requests,\n",
    "                \"input_tokens\": cb.prompt_tokens,\n",
    "                \"output_tokens\": cb.completion_tokens,\n",
    "                \"total_tokens\": cb.total_tokens\n",
    "            }\n",
    "            \n",
    "            # Update state with token usage\n",
    "            update_usage(self.state, usage_dict)\n",
    "            \n",
    "            # Log token usage for this interaction\n",
    "            if cb.successful_requests > 0:\n",
    "                logger.info(\n",
    "                    f\"üî¢ LangGraph interaction - Input: {cb.prompt_tokens}, \"\n",
    "                    f\"Output: {cb.completion_tokens}, Total: {cb.total_tokens} tokens \"\n",
    "                    f\"({cb.successful_requests} requests) - Cost: ${cb.total_cost:.4f}\"\n",
    "                )\n",
    "        \n",
    "        # Extract the AI response content for return\n",
    "        response_content = \"\"\n",
    "        if responses.get(\"messages\"):\n",
    "            # Get the last AI message from the response\n",
    "            for message in reversed(responses[\"messages\"]):\n",
    "                if hasattr(message, 'content') and message.content and message.__class__.__name__ == \"AIMessage\":\n",
    "                    response_content = message.content\n",
    "                    break\n",
    "        \n",
    "        # Return both response and usage for the caller to use\n",
    "        return response_content, usage_dict\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main chat loop for the orchestrator agent.\n",
    "        \"\"\"\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not set!\")\n",
    "            print(\"Set it with: export OPENAI_API_KEY='your-key-here'\")\n",
    "            response = input(\"Continue anyway? (y/n): \")\n",
    "            if response.lower() != 'y':\n",
    "                sys.exit(1)\n",
    "        \n",
    "        print(\"\\nü§ñ LangGraph Multi-Agent System Started!\")\n",
    "        print(\"Memory commands: 'history', 'clear memory', 'memory summary'\")\n",
    "        print(\"Type 'exit', 'quit', or press Ctrl+C to quit\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(\"\\n\" + \"-\" * 40)\n",
    "                user_input = input(\"üí¨ You > \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Check for exit commands\n",
    "                if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "                    print(\"\\nüëã Goodbye! Thanks for using the AI-powered chat.\")\n",
    "                    break\n",
    "\n",
    "                # Track the state BEFORE this turn for usage calculation\n",
    "                prev_requests = self.state.requests\n",
    "                prev_input = self.state.input_tokens\n",
    "                prev_output = self.state.output_tokens\n",
    "                prev_total = self.state.total_tokens\n",
    "                \n",
    "                # Process the user input through the graph\n",
    "                try:\n",
    "                    response, usage_dict = self.main_graph_invoke(user_input, self.session_id)\n",
    "                    print(f\"\\nü§ñ Assistant > {response}\")\n",
    "                    \n",
    "                    # Calculate and display turn usage if there was LLM activity\n",
    "                    turn_requests = self.state.requests - prev_requests\n",
    "                    turn_input = self.state.input_tokens - prev_input\n",
    "                    turn_output = self.state.output_tokens - prev_output\n",
    "                    turn_total = self.state.total_tokens - prev_total\n",
    "                    \n",
    "                    if turn_requests > 0:\n",
    "                        # Calculate turn cost\n",
    "                        turn_input_cost = (turn_input / 1_000_000) * self.state.input_token_price_per_million\n",
    "                        turn_output_cost = (turn_output / 1_000_000) * self.state.output_token_price_per_million\n",
    "                        turn_cost = turn_input_cost + turn_output_cost\n",
    "                        \n",
    "                        # Calculate cumulative cost\n",
    "                        total_cost = calculate_cost(self.state)\n",
    "                        logger.info(f\"\\nüìä This turn: {turn_input:,} in, {turn_output:,} out, {turn_total:,} total tokens ({turn_requests} requests) - Cost: ${turn_cost:.4f}\")\n",
    "                        logger.info(f\"\\nüìä Cumulative: {self.state.input_tokens:,} in, {self.state.output_tokens:,} out, {self.state.total_tokens:,} total tokens ({self.state.requests} requests) - Total cost: ${total_cost:.4f}\")\n",
    "                    \n",
    "                except Exception as graph_error:\n",
    "                    print(f\"\\n‚ùå Error processing request: {graph_error}\")\n",
    "                    self.logger.error(f\"Graph processing error: {graph_error}\", exc_info=True)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n‚ö†Ô∏è  Interrupted! Type 'exit' to quit or continue chatting.\")\n",
    "                # Display final usage stats on interrupt\n",
    "                if self.state.requests > 0:\n",
    "                    print(f\"\\n{get_usage_summary(self.state)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "                self.logger.error(f\"REPL error: {e}\", exc_info=True)\n",
    "        \n",
    "        # Display final usage stats on exit\n",
    "        if self.state.requests > 0:\n",
    "            print(f\"\\n{get_usage_summary(self.state)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4db87",
   "metadata": {},
   "source": [
    "### CIF/gradio_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd70cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:18:58 - core.logging_utils - INFO - Logging initialized - logs directory: c:\\Users\\kakao\\Desktop\\AI_Agent\\Semantic Layer\\logs\n",
      "10:18:58 - core.logging_utils - INFO - Log rotation configured: max 10.0MB per file, keeping 5 backups\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LLM-Powered Interactive Chat System with Tool Selection\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from core import logging_utils, utils\n",
    "\n",
    "log_level_str = os.environ.get(\"LOG_LEVEL\", \"INFO\").upper()\n",
    "log_levels = {\n",
    "    \"DEBUG\": logging.DEBUG,\n",
    "    \"INFO\": logging.INFO,\n",
    "    \"WARNING\": logging.WARNING,\n",
    "    \"ERROR\": logging.ERROR,\n",
    "    \"CRITICAL\": logging.CRITICAL\n",
    "}\n",
    "log_level = log_levels.get(log_level_str, logging.INFO)\n",
    "\n",
    "logging_utils.setup_logging(\n",
    "    log_dir=\"logs\",\n",
    "    log_level=logging.INFO,\n",
    "    max_bytes=10 * 1024 * 1024, \n",
    "    backup_count=5, \n",
    "    console_output=True \n",
    ")\n",
    "\n",
    "logging_utils.cleanup_old_logs(log_dir=\"logs\", days_to_keep=30)\n",
    "logger = logging_utils.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4b883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:19:00 - __main__ - INFO - Initializing SQLite checkpointer at: memory\\langgraph_checkpoints.db\n",
      "10:19:00 - __main__ - INFO - SQLite checkpointer initialized successfully\n",
      "10:19:00 - __main__ - INFO - Persistent memory enabled - checkpointer initialized\n",
      "10:19:00 - __main__ - INFO - ContentSafetyAgent initialized.\n",
      "c:\\Users\\kakao\\miniforge3\\envs\\agent_env\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:2073: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "10:19:00 - __main__ - INFO - RoutingAgent initialized.\n",
      "10:19:00 - __main__ - INFO - LegalExpertAgent initialized.\n",
      "10:19:00 - __main__ - INFO - GeneralAgent initialized - tools will be created per run with state context.\n",
      "10:19:00 - __main__ - INFO - SafetyTerminationAgent initialized.\n",
      "10:19:00 - __main__ - INFO - OrchestratorAgent initialized with persistent memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ LangGraph Multi-Agent System Started!\n",
      "Memory commands: 'history', 'clear memory', 'memory summary'\n",
      "Type 'exit', 'quit', or press Ctrl+C to quit\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "# from orchestrator_agent import OrchestratorAgent\n",
    "# from state import get_usage_summary\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point for the AI-powered interactive chat\"\"\"\n",
    "\n",
    "    session_id = \"my_session\"\n",
    "    orchestrator_agent = OrchestratorAgent(session_id=session_id)\n",
    "    \n",
    "    try:\n",
    "        orchestrator_agent.run()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Goodbye! Thanks for using the AI-powered chat.\")\n",
    "        if hasattr(orchestrator_agent, 'state') and orchestrator_agent.state.requests > 0:\n",
    "            print(f\"\\n{get_usage_summary(orchestrator_agent.state)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {e}\", exc_info=True)\n",
    "        print(f\"\\n‚ùå Fatal error: {e}\")\n",
    "        if hasattr(orchestrator_agent, 'state') and orchestrator_agent.state.requests > 0:\n",
    "            print(f\"\\n{get_usage_summary(orchestrator_agent.state)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0dc96",
   "metadata": {},
   "source": [
    "### CIF/gradio_interface.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4358dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kakao\\miniforge3\\envs\\agent_env\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:2073: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# [1] Agent Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (Í∏∞Ï°¥ ÌÅ¥ÎûòÏä§ ÌôúÏö©)\n",
    "# ÏÑ∏ÏÖò IDÎäî ÎÖ∏Ìä∏Î∂Å ÌôòÍ≤ΩÏóê ÎßûÏ∂∞ Í≥†Ï†ïÌïòÍ±∞ÎÇò ÎèôÏ†ÅÏúºÎ°ú ÏÉùÏÑ±\n",
    "agent_instance = OrchestratorAgent(session_id=\"notebook_interactive_session\")\n",
    "\n",
    "def chat_adapter(message: str, history: List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    GradioÏùò ÏûÖÎ†•(message)ÏùÑ OrchestratorAgent.main_graph_invokeÎ°ú Ï†ÑÎã¨ÌïòÍ≥†\n",
    "    Í≤∞Í≥ºÎ•º Î∞òÌôòÌïòÎäî Ïñ¥ÎåëÌÑ∞ Ìï®Ïàò\n",
    "    \"\"\"\n",
    "    if not message:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # LangGraph Ïã§Ìñâ (Îã®Ïùº ÌÑ¥ Ï≤òÎ¶¨)\n",
    "        # response_content: Ïã§Ï†ú ÎãµÎ≥Ä ÌÖçÏä§Ìä∏\n",
    "        # usage_dict: ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ Ï†ïÎ≥¥\n",
    "        response_content, usage_dict = agent_instance.main_graph_invoke(\n",
    "            user_input=message,\n",
    "            session_id=agent_instance.session_id\n",
    "        )\n",
    "\n",
    "        # (ÏÑ†ÌÉù ÏÇ¨Ìï≠) ÎîîÎ≤ÑÍπÖÏö© ÏÇ¨Ïö©Îüâ Ï†ïÎ≥¥Î•º ÎãµÎ≥Ä ÌïòÎã®Ïóê ÏûëÍ≤å Ï∂îÍ∞Ä\n",
    "        usage_info = (\n",
    "            f\"\\n\\n---\\n\"\n",
    "            f\"üìä **Usage:** {usage_dict.get('total_tokens', 0)} tokens \"\n",
    "            f\"(${usage_dict.get('total_cost', 0.0):.4f})\"\n",
    "        )\n",
    "        \n",
    "        # ÏàúÏàò ÎãµÎ≥ÄÎßå ÏõêÌïòÎ©¥ usage_info Ï†úÍ±∞ Í∞ÄÎä•\n",
    "        return response_content + usage_info\n",
    "\n",
    "    except Exception as e:\n",
    "        # ÏóêÎü¨ Î∞úÏÉù Ïãú UIÏóê Î∂âÏùÄÏÉâÏúºÎ°ú ÌëúÏãú\n",
    "        error_msg = f\"‚ùå **Error:** {str(e)}\"\n",
    "        agent_instance.logger.error(f\"Gradio Adapter Error: {e}\", exc_info=True)\n",
    "        return error_msg\n",
    "\n",
    "# [2] Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "# ChatInterfaceÎäî ÏµúÏã† Gradio Î≤ÑÏ†ÑÏóê ÏµúÏ†ÅÌôîÎêú Ï±ÑÌåÖ Ï†ÑÏö© UIÏûÖÎãàÎã§.\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_adapter,\n",
    "    title=\"Ïã†Ïö©Î∂ÑÏÑù ÏóêÏù¥Ï†ÑÌä∏\",\n",
    "    description=\"ÏóêÏù¥Ï†ÑÌä∏ ÌÖåÏä§Ìä∏ ÌôòÍ≤Ω [gpt-3.5-turbo Í∏∞Î∞ò]\",\n",
    "    examples=[\"ÏïàÎÖï, ÎÑå ÎàÑÍµ¨Îãà?\", \"Î≤ïÎ•† ÏûêÎ¨∏Ïù¥ ÌïÑÏöîÌï¥.\", \"ÏãúÏä§ÌÖú Î©îÎ™®Î¶¨ ÏÉÅÌÉú Î≥¥Ïó¨Ï§ò.\"],\n",
    ")\n",
    "\n",
    "# [3] Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ïã§Ìñâ\n",
    "# inline=True: ÏÖÄ Ï∂úÎ†•Ï∞ΩÏóê Î∞îÎ°ú Î†åÎçîÎßÅ\n",
    "# share=False: Î°úÏª¨ÏóêÏÑúÎßå Ï†ëÍ∑º (Î≥¥Ïïà)\n",
    "demo.launch(inline=True, share=False, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
