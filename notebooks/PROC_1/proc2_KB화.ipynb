{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a22f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 워크플로우의 초기는 다음과 같이 생긴다\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "from entity.process import GuardCondition, TaskSpec, TaskType, Layer, AgentNature, AgentRole, Process\n",
    "from entity.validators import TokenValidator, SpecChainValidator\n",
    "from entity.tokens import Token\n",
    "from core.utils import load_resource_specs\n",
    "import os\n",
    "import logging\n",
    "from core import logging_utils\n",
    "from pathlib import Path\n",
    "from core.tokenDB import TokenRepository\n",
    "from core.utils import load_cfg\n",
    "\n",
    "# 0. master config 불러오기\n",
    "cfg = load_cfg(Path().parent.parent / \"cfg\" / \"prod.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef41c0a",
   "metadata": {},
   "source": [
    "### 0. 노트북 작성목적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce0dae",
   "metadata": {},
   "source": [
    "PROC_1 - 청킹 및 단순 적재 (Hard Process)\n",
    "(금융연수원_소호_여신심사.md와 금융연수원_여신심사 및 관리.md)를 읽어들어, T1_LOAD, T1_SPLT, T1_TAG, T1_VECT의 태스크를 통해 \"지식체계화\"하는 것을 목표로 둔다. 즉, 최초로 형성되는 \"비정형 지식 데이터베이스\"의 자동화 프로세스 시도인 것이다.   \n",
    "  \n",
    "본 프로세스는 전부 비LLM함수로만 진행되는 단순 적재 Hard Process이다. \n",
    "지식체계화는 Proc_2, Proc_3, Proc_3에서 시도한다. \n",
    "\n",
    "내부 함수화, 즉 MCP_proxy는 스크립트 단위로 관리된다.  \n",
    "현재 Hard Process를 불러오는 것도 없으니깐, 그것부터 정의를 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30f2be",
   "metadata": {},
   "source": [
    "### 1. 최초 로드 및 DB 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678a69",
   "metadata": {},
   "source": [
    "### 2. ./Tools/tools:TokenizationPipeline 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01716c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_files(data_path: str = \"./data/input\") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Update: 파일 경로가 들어오면 해당 파일만, 폴더가 들어오면 폴더 내 md 전체 로드\n",
    "    \"\"\"\n",
    "    target_path = Path(data_path)\n",
    "    \n",
    "    # 1. 존재 여부 확인\n",
    "    if not target_path.exists():\n",
    "        print(f\"[Error] 경로가 존재하지 않음: {data_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Case A: 단일 파일인 경우\n",
    "    if target_path.is_file():\n",
    "        if target_path.suffix.lower() != '.md':\n",
    "            print(\"[Warn] .md 파일이 아님\")\n",
    "            return {}\n",
    "        return {target_path.stem: target_path.read_text(encoding=\"utf-8\")}\n",
    "\n",
    "    # 3. Case B: 디렉토리인 경우\n",
    "    if target_path.is_dir():\n",
    "        return {\n",
    "            file.stem: file.read_text(encoding=\"utf-8\")\n",
    "            for file in target_path.glob(\"*.md\")\n",
    "        }\n",
    "\n",
    "    return {}\n",
    "\n",
    "class TokenizationPipeline:\n",
    "    def __init__(self, db_repo: TokenRepository, chunk_size: int = 500, overlap: int = 50):\n",
    "        self.repo = db_repo\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _guess_topic(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"[Heuristic] 텍스트 키워드 기반 초기 Topic 추정 (LLM 전 단계)\"\"\"\n",
    "        topics = {}\n",
    "\n",
    "        # 향후 설정 필요\n",
    "        if \"매출\" in text or \"이익\" in text: topics[\"TOPIC_FINANCE\"] = 0.8\n",
    "        if \"담보\" in text or \"보증\" in text: topics[\"TOPIC_COLLATERAL\"] = 0.8\n",
    "        if \"소송\" in text or \"연체\" in text: topics[\"TOPIC_RISK\"] = 0.9\n",
    "        \n",
    "        if not topics: topics[\"TOPIC_GENERAL\"] = 0.5\n",
    "        return topics\n",
    "\n",
    "    def _create_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"Fixed Size + Overlap 기반 청킹\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_len = len(text)\n",
    "        \n",
    "        while start < text_len:\n",
    "            end = min(start + self.chunk_size, text_len)\n",
    "            chunks.append(text[start:end])\n",
    "            if end == text_len: break\n",
    "            start += (self.chunk_size - self.overlap) # Overlap 이동\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def run(self, data_path: str):\n",
    "        \"\"\"Pipeline Execution: Load -> Split -> Tokenize -> Save\"\"\"\n",
    "        # 1. Load\n",
    "        raw_docs = load_markdown_files(data_path)\n",
    "        if not raw_docs:\n",
    "            print(\"[Info] 처리할 문서가 없습니다.\")\n",
    "            return\n",
    "\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for source_id, text in raw_docs.items():\n",
    "            # 2. Split\n",
    "            chunks = self._create_chunks(text)\n",
    "            \n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                # 3. Tokenize (Process Objectification)\n",
    "                token = Token(\n",
    "                    trace_id=f\"TKN_{source_id}_{idx:04d}\", # 순서 보장 ID\n",
    "                    source_id=source_id,\n",
    "                    history=[\"LOAD_SPLIT\"],\n",
    "                    content={\n",
    "                        \"text\": chunk_text,\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                        \"length\": len(chunk_text),\n",
    "                        \"content_ref\" : str(Path(data_path) / f\"{source_id}.md\")\n",
    "                    },\n",
    "                    topics=self._guess_topic(chunk_text)\n",
    "                )\n",
    "                \n",
    "                # 4. Save\n",
    "                self.repo.save(token)\n",
    "                total_tokens += 1\n",
    "                \n",
    "        print(f\"[Success] Pipeline Completed. Total Tokens Saved: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d10992",
   "metadata": {},
   "source": [
    "### 3. MD 파일 토큰 형태로 DB 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7391929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] Pipeline Completed. Total Tokens Saved: 461\n"
     ]
    }
   ],
   "source": [
    "repo = TokenRepository(cfg['location']['token_db'])\n",
    "pipeline = TokenizationPipeline(repo, chunk_size=500, overlap=50)\n",
    "pipeline.run(\"./data/input/금융연수원_소호 여신심사.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "265a9107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] Pipeline Completed. Total Tokens Saved: 872\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(\"./data/input/금융연수원_여신심사 및 관리.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.load(\"TKN_금융연수원_소호 여신심사_0311\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
