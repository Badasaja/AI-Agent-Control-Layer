{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5fba35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a22f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 워크플로우의 초기는 다음과 같이 생긴다\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "from entity.process import GuardCondition, TaskSpec, TaskType, Layer, AgentNature, AgentRole, Process\n",
    "from entity.validators import TokenValidator, SpecChainValidator\n",
    "from entity.tokens import Token\n",
    "from core.utils import load_resource_specs\n",
    "from core.tokenDB import TokenRepository\n",
    "import os\n",
    "import logging\n",
    "from core import logging_utils\n",
    "from pathlib import Path\n",
    "from core.tokenDB import TokenRepository\n",
    "from core.utils import load_cfg\n",
    "\n",
    "# 0. master config 불러오기\n",
    "cfg = load_cfg(Path().parent.parent / \"cfg\" / \"prod.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef41c0a",
   "metadata": {},
   "source": [
    "### 0. 노트북 작성목적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce0dae",
   "metadata": {},
   "source": [
    "PROC_1 - 청킹 및 단순 적재 (Hard Process)\n",
    "(금융연수원_소호_여신심사.md와 금융연수원_여신심사 및 관리.md)를 읽어들어, T1_LOAD, T1_SPLT, T1_TAG, T1_VECT의 태스크를 통해 \"지식체계화\"하는 것을 목표로 둔다. 즉, 최초로 형성되는 \"비정형 지식 데이터베이스\"의 자동화 프로세스 시도인 것이다.   \n",
    "  \n",
    "본 프로세스는 전부 비LLM함수로만 진행되는 단순 적재 Hard Process이다. \n",
    "지식체계화는 Proc_2, Proc_3, Proc_3에서 시도한다. \n",
    "\n",
    "내부 함수화, 즉 MCP_proxy는 스크립트 단위로 관리된다.  \n",
    "현재 Hard Process를 불러오는 것도 없으니깐, 그것부터 정의를 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30f2be",
   "metadata": {},
   "source": [
    "### 1. 최초 로드 및 DB 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53678a69",
   "metadata": {},
   "source": [
    "### 2. ./Tools/tools:TokenizationPipeline 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_files(data_path: str = \"./data/input\") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Update: 파일 경로가 들어오면 해당 파일만, 폴더가 들어오면 폴더 내 md 전체 로드\n",
    "    \"\"\"\n",
    "    target_path = Path(data_path)\n",
    "    \n",
    "    # 1. 존재 여부 확인\n",
    "    if not target_path.exists():\n",
    "        print(f\"[Error] 경로가 존재하지 않음: {data_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Case A: 단일 파일인 경우\n",
    "    if target_path.is_file():\n",
    "        if target_path.suffix.lower() != '.md':\n",
    "            print(\"[Warn] .md 파일이 아님\")\n",
    "            return {}\n",
    "        return {target_path.stem: target_path.read_text(encoding=\"utf-8\")}\n",
    "\n",
    "    # 3. Case B: 디렉토리인 경우\n",
    "    if target_path.is_dir():\n",
    "        return {\n",
    "            file.stem: file.read_text(encoding=\"utf-8\")\n",
    "            for file in target_path.glob(\"*.md\")\n",
    "        }\n",
    "\n",
    "    return {}\n",
    "\n",
    "class TokenizationPipeline:\n",
    "    def __init__(self, db_repo: TokenRepository, chunk_size: int = 500, overlap: int = 50):\n",
    "        self.repo = db_repo\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def _guess_topic(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"[Heuristic] 텍스트 키워드 기반 초기 Topic 추정 (LLM 전 단계)\"\"\"\n",
    "        topics = {}\n",
    "\n",
    "        # 향후 설정 필요\n",
    "        if \"매출\" in text or \"이익\" in text: topics[\"TOPIC_FINANCE\"] = 0.8\n",
    "        if \"담보\" in text or \"보증\" in text: topics[\"TOPIC_COLLATERAL\"] = 0.8\n",
    "        if \"소송\" in text or \"연체\" in text: topics[\"TOPIC_RISK\"] = 0.9\n",
    "        \n",
    "        if not topics: topics[\"TOPIC_GENERAL\"] = 0.5\n",
    "        return topics\n",
    "\n",
    "    def _create_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"Fixed Size + Overlap 기반 청킹\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_len = len(text)\n",
    "        \n",
    "        while start < text_len:\n",
    "            end = min(start + self.chunk_size, text_len)\n",
    "            chunks.append(text[start:end])\n",
    "            if end == text_len: break\n",
    "            start += (self.chunk_size - self.overlap) # Overlap 이동\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def run(self, data_path: str):\n",
    "        \"\"\"Pipeline Execution: Load -> Split -> Tokenize -> Save\"\"\"\n",
    "        # 1. Load\n",
    "        raw_docs = load_markdown_files(data_path)\n",
    "        if not raw_docs:\n",
    "            print(\"[Info] 처리할 문서가 없습니다.\")\n",
    "            return\n",
    "\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for source_id, text in raw_docs.items():\n",
    "            # 2. Split\n",
    "            chunks = self._create_chunks(text)\n",
    "            \n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                # 3. Tokenize (Process Objectification)\n",
    "                token = Token(\n",
    "                    trace_id=f\"TKN_{source_id}_{idx:04d}\", # 순서 보장 ID\n",
    "                    source_id=source_id,\n",
    "                    history=[\"LOAD_SPLIT\"],\n",
    "                    content={\n",
    "                        \"text\": chunk_text,\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                        \"length\": len(chunk_text),\n",
    "                        \"content_ref\" : str(Path(data_path) / f\"{source_id}.md\")\n",
    "                    },\n",
    "                    topics=self._guess_topic(chunk_text)\n",
    "                )\n",
    "                \n",
    "                # 4. Save\n",
    "                self.repo.save(token)\n",
    "                total_tokens += 1\n",
    "                \n",
    "        print(f\"[Success] Pipeline Completed. Total Tokens Saved: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d10992",
   "metadata": {},
   "source": [
    "### 3. (Proc_1) MD 파일 토큰 형태로 DB 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7391929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = TokenRepository(cfg['location']['token_db'], table_name=\"SOHO_LOAN_raw_knowledge\")\n",
    "pipeline = TokenizationPipeline(repo, chunk_size=500, overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7830484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] Pipeline Completed. Total Tokens Saved: 461\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(\"./data/input/금융연수원_소호 여신심사.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa6773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] Exported 1333 rows to C:/Users/kakao/Desktop/temp_db/exported_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def export_tokens_to_excel(db_path: str, output_path: str):\n",
    "    if not Path(db_path).exists():\n",
    "        print(f\"[Error] DB file not found: {db_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1. DB Connection\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # 2. Load Data to DataFrame\n",
    "        query = \"SELECT * FROM tokens ORDER BY source_id, trace_id\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # 3. Data Processing (Optional: JSON fields parsing)\n",
    "        # 엑셀에서 보기 편하게 JSON 컬럼은 그대로 텍스트로 둠.\n",
    "        \n",
    "        # 4. Export to CSV (Excel 호환, BOM 추가)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[Success] Exported {len(df)} rows to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Fail] Export error: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # DB 경로와 저장될 파일명 지정\n",
    "    export_tokens_to_excel(\"./db/tb_cspn.db\", \"C:/Users/kakao/Desktop/temp_db/exported_tokens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60e995",
   "metadata": {},
   "source": [
    "### 4. (Proc_2) Knowledge Base 화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3a3e9",
   "metadata": {},
   "source": [
    "단순 메타데이터 필드를 정의해주는 것으로는 끝나선 안된다. 우리가 이 문서를 인식시킴으로써 \"무얼 성취하고자 하는가\"가 본 POC에서 중요하다.  \n",
    "앞서 Semantic_Layer_Seed에서 이야기했던 개념, 우선 손으로 다시 써서 익게 하자.  \n",
    "\n",
    "프로세스에는 Hard / Soft (Context-rich)~(Newborn) / Liquid이 있다. 즉, 우리는 프로세스의 정형화 정도에 따라 \n",
    "프로세스를 분류하고 있는 것이다. 또한, 우리가 읽어들인 것은 결국 절차의 집합소이며, 상당부분은 Liquid일 것이다.  \n",
    "\n",
    "왜 애초에 프로세스를 언어화 했는가?  \n",
    "\n",
    "1. 토큰 기반 통제 가능성이다.  \n",
    "- KPMG가 내리는 전문가의 정의는 \"충분한 데이터가 제공됐을때, 해당 데이터를 전부 고려하여 정해진 절차에 따라 판단을 내릴 수 있는 agent\"이다. \n",
    "- 토큰 기반 invoke는 데이터가 없으면 멈춘다. 사용자에게 요구한다. 환각을 방지한다. \n",
    "\n",
    "2. 추적 가능하다. 모든 세션과 비정형데이터는 토큰이라는 공통 언어로 통제된다. \n",
    "\n",
    "3. 지식이 진화한다. \n",
    "- 각 토큰은 배치를 통해 \"self-study\"되며, 맥락이 풍부해진다. (smart object화)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d6190",
   "metadata": {},
   "source": [
    "#### 4.1. LLM 메서드 필요 (agents.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a7633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5630eb35",
   "metadata": {},
   "source": [
    "### 4.2. 토큰 업데이트 및 Merge 로직 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
